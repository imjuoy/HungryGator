{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import io, time, json\n",
    "\n",
    "##Below block of code is used to extract the data from the Zomato API.\n",
    "\n",
    "headers = {'Content-Type': 'application/json', 'user-key': '592789033e4cf0e384b54c1ec424c706'}\n",
    "offset = 0\n",
    "restaurantNameList = []\n",
    "ratingList = []\n",
    "ratingTextList = []\n",
    "votesList = []\n",
    "totalData = []\n",
    "urlList = []\n",
    "addressList = []\n",
    "sourceList = []\n",
    "while offset < 100:\n",
    "    url = 'https://developers.zomato.com/api/v2.1/search?entity_id=291&entity_type=city&start='+str(offset)+'&count=20'\n",
    "    Response = requests.get(url , headers=headers)\n",
    "    offset += 20\n",
    "    Response_Json = Response.json()\n",
    "    Array_length = Response_Json['restaurants']\n",
    "    for i in range(len(Array_length)):\n",
    "        totalData.append(Response_Json['restaurants'][i])\n",
    "        restaurantNameList.append(Response_Json['restaurants'][i]['restaurant']['name'])\n",
    "        ratingList.append(Response_Json['restaurants'][i]['restaurant']['user_rating']['aggregate_rating'])\n",
    "        ratingTextList.append(Response_Json['restaurants'][i]['restaurant']['user_rating']['rating_text'])\n",
    "        votesList.append(Response_Json['restaurants'][i]['restaurant']['user_rating']['votes'])\n",
    "        urlList.append(Response_Json['restaurants'][i]['restaurant']['url'])\n",
    "        addressList.append(Response_Json['restaurants'][i]['restaurant']['location']['address'])\n",
    "        sourceList.append('Zomato')\n",
    "df1 = pd.DataFrame({'AllData':totalData})\n",
    "df2 = pd.DataFrame({'Restaurant Name':restaurantNameList,'Rating':ratingList, 'Rating Text':ratingTextList, 'Votes':votesList, 'Url':urlList, 'Address': addressList, 'Source': sourceList})\n",
    "writer = pd.ExcelWriter('../DataFiles/Final_Data.xlsx' , engine='xlsxwriter')\n",
    "df1.to_excel(writer,'ZomatoRaw',index=False)\n",
    "df2.to_excel(writer,'ZomatoClean',index=False)\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The below block of code is used to read the API Key required for extracting data from Yelp fusion API.\n",
    "\n",
    "def read_api_key(filepath):\n",
    "    \"\"\"\n",
    "    Read the Yelp API Key from file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (string): File containing API Key\n",
    "    Returns:\n",
    "        api_key (string): The API Key\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('api_key.txt', 'r') as f:\n",
    "        return f.read().replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "{'error': {'code': 'VALIDATION_ERROR', 'description': 'Too many results requested, limit+offset must be <= 1000.'}}\n"
     ]
    }
   ],
   "source": [
    "##The function below is used to retrieve all the restaurants from the YELP API.\n",
    "\n",
    "def all_restaurants(api_key, query):\n",
    "    \"\"\"\n",
    "    Retrieve ALL the restaurants on Yelp for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        results (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    \n",
    "    offsetno=0\n",
    "    total =0\n",
    "    output=[]\n",
    "    headers = {        \n",
    "        \"authorization\": 'Bearer %s' % api_key, # for the yelp API     \n",
    "    }\n",
    "    while (offsetno <= total):\n",
    "        params = { #parameters are case sensitive!\n",
    "            \"location\": query,\n",
    "            \"limit\": 20,\n",
    "            \"offset\": offsetno,\n",
    "            \"categories\": \"restaurants\"\n",
    "            \n",
    "        }\n",
    "    \n",
    "        response = requests.get('https://api.yelp.com/v3/businesses/search',\n",
    "                            headers=headers, params=params)\n",
    "        result = response.json()\n",
    "        try:\n",
    "            output.append( result['businesses'])\n",
    "        except KeyError:\n",
    "            print(result)\n",
    "            return output\n",
    "        offsetno =offsetno + 20\n",
    "        total = result['total']\n",
    "        print (offsetno)\n",
    "        #print (len(output))\n",
    "    return(output)\n",
    "\n",
    "api_key = read_api_key('api_key.txt')\n",
    "data = all_restaurants(api_key, 'Miami')\n",
    "new_data = {}\n",
    "new_data[\"name\"]=[]\n",
    "new_data[\"rating\"]=[]\n",
    "new_data[\"price\"]=[]\n",
    "new_data[\"review_count\"]=[]\n",
    "new_data[\"url\"]=[]\n",
    "new_data[\"category\"]=[]\n",
    "new_data[\"zip\"]=[]\n",
    "alldata=[]\n",
    "for i in data:\n",
    "    for each in i:\n",
    "        new_data[\"name\"].append(each[\"name\"])\n",
    "        new_data[\"rating\"].append(each[\"rating\"])\n",
    "        new_data[\"url\"].append(each[\"url\"])\n",
    "        try:\n",
    "            new_data[\"price\"].append(each[\"price\"])\n",
    "        except KeyError:\n",
    "            new_data[\"price\"].append(None)\n",
    "\n",
    "        new_data[\"review_count\"].append(each[\"review_count\"])\n",
    "        new_data[\"category\"].append(each[\"categories\"][0][\"title\"])\n",
    "        new_data[\"zip\"].append(each[\"location\"][\"zip_code\"])\n",
    "        alldata.append(each)\n",
    "df4 = pd.DataFrame(alldata)\n",
    "df4.to_excel(writer,'YelpRaw',index=False)\n",
    "df3 = pd.DataFrame(new_data)\n",
    "df3.to_excel(writer,'YelpClean',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1\n",
      "40\n",
      "2\n",
      "60\n",
      "3\n",
      "80\n",
      "4\n",
      "100\n",
      "5\n",
      "120\n",
      "6\n",
      "140\n",
      "7\n",
      "160\n",
      "8\n",
      "180\n",
      "9\n",
      "200\n",
      "10\n",
      "220\n",
      "11\n",
      "240\n",
      "12\n",
      "260\n",
      "13\n",
      "280\n",
      "14\n",
      "300\n",
      "15\n",
      "320\n",
      "16\n",
      "340\n",
      "17\n",
      "360\n",
      "18\n",
      "380\n",
      "19\n",
      "400\n",
      "20\n",
      "420\n",
      "21\n",
      "440\n",
      "22\n",
      "460\n",
      "23\n",
      "480\n",
      "24\n",
      "500\n",
      "25\n",
      "520\n",
      "26\n",
      "540\n",
      "27\n",
      "560\n",
      "28\n",
      "580\n",
      "29\n",
      "600\n",
      "30\n",
      "620\n",
      "31\n",
      "640\n",
      "32\n",
      "660\n",
      "33\n",
      "680\n",
      "34\n",
      "700\n",
      "35\n",
      "720\n",
      "36\n",
      "740\n",
      "37\n",
      "760\n",
      "38\n",
      "780\n",
      "39\n",
      "800\n",
      "40\n",
      "820\n",
      "41\n",
      "840\n",
      "42\n",
      "860\n",
      "43\n",
      "880\n",
      "44\n",
      "900\n",
      "45\n",
      "920\n",
      "46\n",
      "940\n",
      "47\n",
      "960\n",
      "48\n",
      "980\n",
      "49\n",
      "1000\n",
      "50\n",
      "{'error': {'code': 'VALIDATION_ERROR', 'description': 'Too many results requested, limit+offset must be <= 1000.'}}\n"
     ]
    }
   ],
   "source": [
    "#df.to_csv(\"Miami_Yelp.csv\",index=False)\n",
    "#df_all = pd.DataFrame(alldata)\n",
    "#df_all.to_csv(\"Miami_raw.csv\",index=False)\n",
    "#df = pd.read_excel(\"/Users/srossgupta/Desktop/Data Focussed Python/HungryGator/DataFiles/Project_prototype_datafiles.xlsx\",sheet_name=\"MergedData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The below code needs to be executed to clean the TripAdvisor CSV file and create a raw and clean dataset out of it.\n",
    "\n",
    "data = pd.read_csv('../DataFiles/TripAdvisor_Data.csv')\n",
    "cleaned_data = data[data[\"City\"]==\"Miami\"]\n",
    "cleaned_data = cleaned_data.loc[:,[\"Restaurant ID\", \"Restaurant URL\",\"Name\",\"Address\",\"Ranking\",\"Total Review\"]]\n",
    "#path = r\"C:/Users/devdi/Desktop/CMU/Final Mini/Python for Data Science/Project/Data.xlsx\"\n",
    "#writer = pd.ExcelWriter(path, engine = 'xlsxwriter')\n",
    "data.to_excel(writer, sheet_name = ' TripAdvisor_raw')\n",
    "cleaned_data.to_excel(writer, sheet_name = 'TripAdvisor_clean')\n",
    "writer.save()\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
